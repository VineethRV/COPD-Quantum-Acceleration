{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264bd23-65a8-4b6b-a19f-94abd98570f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10f717fe-d9ea-444b-a2e0-74ce088cbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./dataset.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=['COPDSEVERITY'])\n",
    "y = df['COPDSEVERITY']\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Handle missing values - fill with median for numeric columns\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "796bacc8-6ed2-47cb-970f-6d9818f65cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 2, 0, 0, 2, 1, 2, 2, 3, 2, 0, 1, 1, 2, 0, 1, 3, 2, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a4a0720-8587-4e84-8101-ed243f9413e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c4cad36-a72a-4877-a2c4-d214c5f0c8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.31667736,  0.43116841, -0.38179255, ..., -0.37796447,\n",
       "        -0.5       , -0.33333333],\n",
       "       [-0.85836489, -0.14436228,  0.7090433 , ..., -0.37796447,\n",
       "        -0.5       , -0.33333333],\n",
       "       [-0.30597321, -0.98847397, -0.51814703, ..., -0.37796447,\n",
       "        -0.5       ,  3.        ],\n",
       "       ...,\n",
       "       [-1.27265865,  1.4095706 , -0.10908358, ..., -0.37796447,\n",
       "        -0.5       , -0.33333333],\n",
       "       [ 1.4202508 ,  1.42875496, -0.38179255, ..., -0.37796447,\n",
       "        -0.5       , -0.33333333],\n",
       "       [ 0.00474712, -0.95010526,  0.0272709 , ...,  2.64575131,\n",
       "        -0.5       , -0.33333333]], shape=(80, 23))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87644284-da1b-44df-a302-80785e3e4070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a8151c9-b1ba-4f15-b58a-5e9ab23f24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c39853b6-56e2-4cba-8d24-cc826e7a2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Neural Network with 2 hidden layers (halving dimensions)\n",
    "input_dim = X_train.shape[1]\n",
    "hidden1_dim = 23\n",
    "hidden2_dim = 11\n",
    "output_dim = len(np.unique(y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "840c9a72-7524-44c0-9872-94c79584525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.fc3 = nn.Linear(hidden2_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9c543f0-cd39-468a-a95a-3f001ffc1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = NeuralNetwork(input_dim, hidden1_dim, hidden2_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "097cb8b4-630c-4c13-9834-8abcb75a1bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5397\n",
      "Epoch [2/100], Loss: 0.4213\n",
      "Epoch [3/100], Loss: 0.4551\n",
      "Epoch [4/100], Loss: 0.5196\n",
      "Epoch [5/100], Loss: 0.4808\n",
      "Epoch [6/100], Loss: 0.5174\n",
      "Epoch [7/100], Loss: 0.4983\n",
      "Epoch [8/100], Loss: 0.5012\n",
      "Epoch [9/100], Loss: 0.4742\n",
      "Epoch [10/100], Loss: 0.4637\n",
      "Epoch [11/100], Loss: 0.5282\n",
      "Epoch [12/100], Loss: 0.5873\n",
      "Epoch [13/100], Loss: 0.5494\n",
      "Epoch [14/100], Loss: 0.4862\n",
      "Epoch [15/100], Loss: 0.4460\n",
      "Epoch [16/100], Loss: 0.5437\n",
      "Epoch [17/100], Loss: 0.4341\n",
      "Epoch [18/100], Loss: 0.5952\n",
      "Epoch [19/100], Loss: 0.5407\n",
      "Epoch [20/100], Loss: 0.4672\n",
      "Epoch [21/100], Loss: 0.4512\n",
      "Epoch [22/100], Loss: 0.5592\n",
      "Epoch [23/100], Loss: 0.4750\n",
      "Epoch [24/100], Loss: 0.3361\n",
      "Epoch [25/100], Loss: 0.4991\n",
      "Epoch [26/100], Loss: 0.4457\n",
      "Epoch [27/100], Loss: 0.4698\n",
      "Epoch [28/100], Loss: 0.5492\n",
      "Epoch [29/100], Loss: 0.5282\n",
      "Epoch [30/100], Loss: 0.4104\n",
      "Epoch [31/100], Loss: 0.4920\n",
      "Epoch [32/100], Loss: 0.4571\n",
      "Epoch [33/100], Loss: 0.4444\n",
      "Epoch [34/100], Loss: 0.3429\n",
      "Epoch [35/100], Loss: 0.5196\n",
      "Epoch [36/100], Loss: 0.4564\n",
      "Epoch [37/100], Loss: 0.5414\n",
      "Epoch [38/100], Loss: 0.5852\n",
      "Epoch [39/100], Loss: 0.4145\n",
      "Epoch [40/100], Loss: 0.4590\n",
      "Epoch [41/100], Loss: 0.4490\n",
      "Epoch [42/100], Loss: 0.5001\n",
      "Epoch [43/100], Loss: 0.4953\n",
      "Epoch [44/100], Loss: 0.4124\n",
      "Epoch [45/100], Loss: 0.3660\n",
      "Epoch [46/100], Loss: 0.3781\n",
      "Epoch [47/100], Loss: 0.4314\n",
      "Epoch [48/100], Loss: 0.4102\n",
      "Epoch [49/100], Loss: 0.4290\n",
      "Epoch [50/100], Loss: 0.4689\n",
      "Epoch [51/100], Loss: 0.4874\n",
      "Epoch [52/100], Loss: 0.5657\n",
      "Epoch [53/100], Loss: 0.4438\n",
      "Epoch [54/100], Loss: 0.4678\n",
      "Epoch [55/100], Loss: 0.3819\n",
      "Epoch [56/100], Loss: 0.3720\n",
      "Epoch [57/100], Loss: 0.4837\n",
      "Epoch [58/100], Loss: 0.5220\n",
      "Epoch [59/100], Loss: 0.4405\n",
      "Epoch [60/100], Loss: 0.4308\n",
      "Epoch [61/100], Loss: 0.4523\n",
      "Epoch [62/100], Loss: 0.4417\n",
      "Epoch [63/100], Loss: 0.4748\n",
      "Epoch [64/100], Loss: 0.4512\n",
      "Epoch [65/100], Loss: 0.3917\n",
      "Epoch [66/100], Loss: 0.5089\n",
      "Epoch [67/100], Loss: 0.4305\n",
      "Epoch [68/100], Loss: 0.4069\n",
      "Epoch [69/100], Loss: 0.4977\n",
      "Epoch [70/100], Loss: 0.5116\n",
      "Epoch [71/100], Loss: 0.4703\n",
      "Epoch [72/100], Loss: 0.4976\n",
      "Epoch [73/100], Loss: 0.4020\n",
      "Epoch [74/100], Loss: 0.4107\n",
      "Epoch [75/100], Loss: 0.3836\n",
      "Epoch [76/100], Loss: 0.4233\n",
      "Epoch [77/100], Loss: 0.3864\n",
      "Epoch [78/100], Loss: 0.3843\n",
      "Epoch [79/100], Loss: 0.3933\n",
      "Epoch [80/100], Loss: 0.4658\n",
      "Epoch [81/100], Loss: 0.4023\n",
      "Epoch [82/100], Loss: 0.4759\n",
      "Epoch [83/100], Loss: 0.4200\n",
      "Epoch [84/100], Loss: 0.4629\n",
      "Epoch [85/100], Loss: 0.4565\n",
      "Epoch [86/100], Loss: 0.4344\n",
      "Epoch [87/100], Loss: 0.3708\n",
      "Epoch [88/100], Loss: 0.3491\n",
      "Epoch [89/100], Loss: 0.4052\n",
      "Epoch [90/100], Loss: 0.5514\n",
      "Epoch [91/100], Loss: 0.3653\n",
      "Epoch [92/100], Loss: 0.3830\n",
      "Epoch [93/100], Loss: 0.4577\n",
      "Epoch [94/100], Loss: 0.3666\n",
      "Epoch [95/100], Loss: 0.3324\n",
      "Epoch [96/100], Loss: 0.4282\n",
      "Epoch [97/100], Loss: 0.4467\n",
      "Epoch [98/100], Loss: 0.3591\n",
      "Epoch [99/100], Loss: 0.3548\n",
      "Epoch [100/100], Loss: 0.4301\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "211c33be-854b-44c1-b6ae-e9673b9b4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.7619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    acc = accuracy_score(y_test_tensor.numpy(), predicted.numpy())\n",
    "    print(f'\\nAccuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acdc51-0704-4c42-8074-082b5e820e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'\\nModel Architecture:')\n",
    "print(f'Input Layer: {input_dim} neurons')\n",
    "print(f'Hidden Layer 1: {hidden1_dim} neurons')\n",
    "print(f'Hidden Layer 2: {hidden2_dim} neurons')\n",
    "print(f'Output Layer: {output_dim} neurons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65acff8c-1744-49a5-91d9-748fa1dab9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane\n",
      "  Downloading pennylane-0.44.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pennylane-lightning\n",
      "  Downloading pennylane_lightning-0.44.0-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from pennylane) (1.16.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from pennylane) (3.6.1)\n",
      "Collecting rustworkx>=0.14.0 (from pennylane)\n",
      "  Downloading rustworkx-0.17.1-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting autograd (from pennylane)\n",
      "  Downloading autograd-1.8.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting appdirs (from pennylane)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting autoray==0.8.2 (from pennylane)\n",
      "  Downloading autoray-0.8.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting cachetools (from pennylane)\n",
      "  Downloading cachetools-6.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from pennylane) (2.32.4)\n",
      "Collecting tomlkit (from pennylane)\n",
      "  Downloading tomlkit-0.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from pennylane) (4.14.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from pennylane) (25.0)\n",
      "Collecting diastatic-malt (from pennylane)\n",
      "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from pennylane) (2.2.6)\n",
      "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning)\n",
      "  Downloading scipy_openblas32-0.3.30.443.1-py3-none-win_amd64.whl.metadata (56 kB)\n",
      "Collecting astunparse (from diastatic-malt->pennylane)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting gast (from diastatic-malt->pennylane)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting termcolor (from diastatic-malt->pennylane)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from requests->pennylane) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from requests->pennylane) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from requests->pennylane) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\miniconda3\\envs\\eda\\lib\\site-packages (from requests->pennylane) (2025.10.5)\n",
      "Downloading pennylane-0.44.0-py3-none-any.whl (5.3 MB)\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.3 MB 854.2 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/5.3 MB 1.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.3/5.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.6/5.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.8/5.3 MB 1.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 2.1/5.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 2.4/5.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 2.6/5.3 MB 1.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.9/5.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.1/5.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.1/5.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.4/5.3 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.9/5.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.9/5.3 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.2/5.3 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.3 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.3 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.3/5.3 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading autoray-0.8.2-py3-none-any.whl (935 kB)\n",
      "   ---------------------------------------- 0.0/935.6 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 524.3/935.6 kB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 786.4/935.6 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 935.6/935.6 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading pennylane_lightning-0.44.0-cp313-cp313-win_amd64.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.0/5.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.8/5.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.6/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.9/5.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 3.4/5.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.2/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading rustworkx-0.17.1-cp39-abi3-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 1.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.0/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading scipy_openblas32-0.3.30.443.1-py3-none-win_amd64.whl (7.1 MB)\n",
      "   ---------------------------------------- 0.0/7.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/7.1 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.0/7.1 MB 2.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.3/7.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.3/7.1 MB 2.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.3/7.1 MB 2.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.6/7.1 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.8/7.1 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.1/7.1 MB 1.2 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.1/7.1 MB 1.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.4/7.1 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 2.6/7.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.1/7.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 3.1/7.1 MB 1.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.4/7.1 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 3.9/7.1 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.5/7.1 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 4.7/7.1 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.2/7.1 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/7.1 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.5/7.1 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.8/7.1 MB 1.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 6.3/7.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.6/7.1 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.8/7.1 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.1/7.1 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading autograd-1.8.0-py3-none-any.whl (51 kB)\n",
      "Downloading cachetools-6.2.4-py3-none-any.whl (11 kB)\n",
      "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tomlkit-0.14.0-py3-none-any.whl (39 kB)\n",
      "Installing collected packages: appdirs, tomlkit, termcolor, scipy-openblas32, rustworkx, gast, cachetools, autoray, autograd, astunparse, diastatic-malt, pennylane-lightning, pennylane\n",
      "\n",
      "   --- ------------------------------------  1/13 [tomlkit]\n",
      "   --------- ------------------------------  3/13 [scipy-openblas32]\n",
      "   ------------ ---------------------------  4/13 [rustworkx]\n",
      "   ------------------ ---------------------  6/13 [cachetools]\n",
      "   --------------------- ------------------  7/13 [autoray]\n",
      "   ------------------------ ---------------  8/13 [autograd]\n",
      "   ------------------------------ --------- 10/13 [diastatic-malt]\n",
      "   ------------------------------ --------- 10/13 [diastatic-malt]\n",
      "   ------------------------------ --------- 10/13 [diastatic-malt]\n",
      "   --------------------------------- ------ 11/13 [pennylane-lightning]\n",
      "   --------------------------------- ------ 11/13 [pennylane-lightning]\n",
      "   --------------------------------- ------ 11/13 [pennylane-lightning]\n",
      "   --------------------------------- ------ 11/13 [pennylane-lightning]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ------------------------------------ --- 12/13 [pennylane]\n",
      "   ---------------------------------------- 13/13 [pennylane]\n",
      "\n",
      "Successfully installed appdirs-1.4.4 astunparse-1.6.3 autograd-1.8.0 autoray-0.8.2 cachetools-6.2.4 diastatic-malt-2.15.2 gast-0.7.0 pennylane-0.44.0 pennylane-lightning-0.44.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.443.1 termcolor-3.3.0 tomlkit-0.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pennylane pennylane-lightning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
